{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cf97094",
   "metadata": {},
   "source": [
    "##### **Retrieval-Augmented Medical Chatbot - High-Fidelity Prototype**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d21ec5",
   "metadata": {},
   "source": [
    "**<span style=\"color: #A1AEB1;\">Retrieval-Augmented Medical Chatbot Pipeline Architecture</span>**\n",
    "\n",
    "| Module                                        | Description                                                                                           | Inputs               | Outputs                           |\n",
    "| --------------------------------------------- | ----------------------------------------------------------------------------------------------------- | -------------------- | --------------------------------- |\n",
    "| **Document Ingestion**                        | Loads documents, extracts text, filenames, timestamps, and page-level metadata.                       | PDF / DOCX / HTML    | Clean Text + Metadata             |\n",
    "| **Preprocessing & Chunking**                  | Normalizes text, removes boilerplate, creates overlapping chunks with rich metadata.                  | Page-level Text      | Text Chunks + Metadata            |\n",
    "| **Embedding & Vectorization**                 | Converts chunks to embeddings using OpenAI Embedding model, then stores in vector database.           | Text Chunks          | Vector Store Entries              |\n",
    "| **Retriever & Re-Ranking**                    | Performs semantic search, applies metadata filters, reranks results using cross-encoder.              | Query + Vector Store | Top-K Passages + Relevance Scores |\n",
    "| **RAG Prompting**                             | Constructs RAG prompt with system rules, retrieved passages, citations, and the user query.           | Top-k Passages       | Prompt for LLM                    |\n",
    "| **LLM Generation**                            | Produces draft medical response with citations and confidence.                                        | Structured Prompt    | Generated Answer                  |\n",
    "| **Safety & Clinical Guardrails**              | Checks for hallucination, unsafe content, missing citations, or clinical risks; may trigger fallback. | Generated Answer     | Safe Final Answer                 |\n",
    "| **Audit Logging & Monitoring**                | Logs queries, retrievals, sources, and model responses for compliance.                                | System Events        | Audit Logs                        |\n",
    "| **Evaluation & Feedback Loop**                | Measures accuracy, safe-fail rates, and retrieval quality to improve pipeline.                        | Logs + Responses     | Metrics & Model Improvements      |\n",
    "\n",
    "<br />\n",
    "\n",
    "<div style=\"text-align: justify;\">\n",
    "\n",
    "<span style=\"color: #D0312D;\">**Note: The Retrieval-Augmented Generation (RAG) pipeline is currently being implemented as an early prototype. It does not yet contain every element specified in the final architecture, even though it illustrates the essential workflow from document ingestion to response generation. Later stages of development will include features like PHI de-identification, safety and compliance layers, improved retrieval optimisation, and thorough assessment systems.**</span>\n",
    "\n",
    "</div>\n",
    "\n",
    "**<span style=\"color: #A1AEB1;\">Retrieval-Augmented Generation Framework Research Summary</span>**\n",
    "\n",
    "<div style=\"text-align: justify;\">\n",
    "\n",
    "Throughout the research into open-source Retrieval-Augmented Generation (RAG) frameworks, I evaluated several leading tools including LangChain, LlamaIndex, Haystack, FlexRAG, and UltraRAG based on factors such as ecosystem maturity, retrieval performance, orchestration flexibility, integration support, scalability, and suitability for medical domain requirements.\n",
    "\n",
    "**LlamaIndex** stood out for its data-centric architecture, offering efficient document ingestion, structured indexing, and advanced query engines such as Tree and Graph indexes. It is excellent for rapid prototyping and scenarios where flexible data connectors are needed. However, it provides less orchestration control for multi-step conversational flows or agent-like reasoning.\n",
    "\n",
    "**Haystack** demonstrated strengths in production-grade pipelines, robust hybrid retrieval (sparse + dense), and a modular architecture that fits enterprise settings well. Its pipeline API is powerful but can be heavier to configure and less ideal for agent-based interactions or dynamic tool-calling workflows.\n",
    "\n",
    "**FlexRAG** and **UltraRAG** offer innovative retrieval strategies and automation capabilities. FlexRAG focuses on dynamic retrieval orchestration, while UltraRAG emphasizes pipeline optimization and evaluation tooling. Despite their promise, both frameworks are still relatively new, with smaller ecosystems, limited documentation, and fewer real-world examples.\n",
    "\n",
    "<span style=\"color: #74B72E;\">**LangChain** turned out to be the most sensible and adaptable alternative for this project when all the choices had been evaluated. It provides flexible workflow orchestration, strong support for tools, agents, and memory components, and broad integration with vector stores and LLM providers. LangChain also enables large-scale ingestion, distributed processing, detailed control over embedding/vectorstore layers, and excellent OpenAI integration, which is important for medical RAG systems that require reliability and strict compliance handling. While LangChain may not include the most optimized retriever internally, its compatibility with external retrieval engines makes it the best fit for building a medically safe, extensible, and future-proof RAG-based medical chatbot.</span>\n",
    "\n",
    "</div>\n",
    "\n",
    "**<span style=\"color: #A1AEB1;\">Retrieval-Augmented Generation Large Language Model (LLM) Research Summary</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4826de4c",
   "metadata": {},
   "source": [
    "##### **Retrieval-Augmented Medical Chatbot - Implemented Code**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94ed3fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant Variables\n",
    "VECTOR_DATABASE_PATH = \"./vector_database/db_faiss\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f7b3a3",
   "metadata": {},
   "source": [
    "**<span style=\"color: #A1AEB1;\">Document Ingestion (Document Loader)</span>**\n",
    "\n",
    "<div style=\"text-align: justify;\">\n",
    "\n",
    "Through the research phase, I noticed that medical dataset often come in multiple formats such as PDF guidelines, DOCX clinical reports, TXT notes, HTML webpages, and more, which implies that each file type requires different preprocessing steps, and handling them individually can introduce errors, inconsistencies, and maintenance challenges. Therefore, a unified approach was necessary to streamline document loading while ensuring that all formats are processed correctly and consistently, and **Document Loader Factory** was implemented to address this need. In brief, it abstracts away file-specific parsing logic while ensuring that every document is transformed into a standardised structure suitable for chunking, embedding, and retrieval. This makes the overall RAG pipeline more robust, extensible, and easier to maintain.\n",
    "\n",
    "Overall, the key reasons for implementing it is to ensure various file formats and future formats are process realiably, extensibility, consistent downstream processing, cleaner architecture, and reusability. Meanwhile, I believe that it is a foundational component that prepares unstructured medical data for high-quality retrieval in the RAG medical chatbot.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40379599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jkhang/Documents/GitHub/Artificial-Intelligence-Medical-Chatbot/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/jkhang/Documents/GitHub/Artificial-Intelligence-Medical-Chatbot/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Framework Libraries\n",
    "from langchain.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b89005b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Loader\n",
    "class DocumentLoader:\n",
    "    def __init__(self):\n",
    "        self.dataset_path = \"./data/\"\n",
    "        self.docs_loaders = {\"*pdf\": PyPDFLoader, \"*docx\": Docx2txtLoader, \"*txt\": TextLoader} \n",
    "    \n",
    "    def load_documents(self):\n",
    "        docs = []\n",
    "        \n",
    "        for file_type, loader_cls in self.docs_loaders.items():\n",
    "            file_paths = glob.glob(f\"{self.dataset_path}/{file_type}\")\n",
    "            \n",
    "            for file_path in file_paths:\n",
    "                docs_loader = loader_cls(file_path)\n",
    "                docs.extend(docs_loader.load())\n",
    "                \n",
    "        return docs   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3936116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of PDF pages: 759\n"
     ]
    }
   ],
   "source": [
    "# Dataset Preparation\n",
    "docs_loaders = DocumentLoader()\n",
    "medical_docs = docs_loaders.load_documents()\n",
    "\n",
    "# Dataset Checking\n",
    "print(f\"Number of PDF pages: {len(medical_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bfc924",
   "metadata": {},
   "source": [
    "**<span style=\"color: #A1AEB1;\">Preprocessing & Chunking</span>**\n",
    "\n",
    "<div style=\"text-align: justify;\">\n",
    "\n",
    "In comparison to more naive or fixed-window chunking methods, the **RecursiveCharacterTextSplitter** produces segments that better preserve semantic continuity, reduce unnatural text breakpoints, and align effectively with the processing requirements of downstream embedding models. Its recursive, rule-based splitting mechanism enables the generation of structurally coherent chunks while maintaining compatibility with LangChain’s document processing pipeline.\n",
    "\n",
    "<span style=\"color: #74B72E;\">Based on the evaluation across multiple chunking strategies applied to medical text corpora, a configuration of **500-character chunk size** with a **50-character overlap** emerged as the most suitable for this RAG implementation. A chunk size of approximately 500 characters provides a strong balance between semantic density and processing efficiency, typically encapsulating one to two clinically meaningful paragraphs. This reduces the likelihood of fragmenting medically significant concepts, thereby improving embedding representativeness and enhancing retrieval precision.</span>\n",
    "\n",
    "The 50-character overlap further addresses boundary effects by ensuring that clinical statements or sentence fragments located near chunk edges are preserved across adjacent segments. This approach minimizes semantic loss without introducing excessive redundancy in the vector store.\n",
    "\n",
    "In short, this chunking configuration offers an effective trade-off—maintaining semantic coherence, optimizing embedding quality, and supporting efficient storage and retrieval—making it well-aligned with best practices for retrieval-augmented generation (RAG) in the medical domain.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9fe58c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Framework Libraries\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter                                 # Split the whole document which containing all text into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1520250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'GPL Ghostscript 9.10', 'creator': '', 'creationdate': '2017-05-01T10:37:35-07:00', 'moddate': '2017-05-01T10:37:35-07:00', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'source': './data/The-Glae-Encyclopedia-of-Medicine.pdf', 'total_pages': 759, 'page': 0, 'page_label': '1'}, page_content='The GALE\\nENCYCLOPEDIA\\nof MEDICINE\\nSECOND EDITION'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.10', 'creator': '', 'creationdate': '2017-05-01T10:37:35-07:00', 'moddate': '2017-05-01T10:37:35-07:00', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'source': './data/The-Glae-Encyclopedia-of-Medicine.pdf', 'total_pages': 759, 'page': 1, 'page_label': '2'}, page_content='The G ALE\\nENCYCLOPEDIA\\nof MEDICINE\\nSECOND EDITION\\nJACQUELINE L. LONGE, EDITOR\\nDEIRDRE S. BLANCHFIELD, ASSOCIATE EDITOR\\nVOLUME\\nC-F\\n2'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.10', 'creator': '', 'creationdate': '2017-05-01T10:37:35-07:00', 'moddate': '2017-05-01T10:37:35-07:00', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'source': './data/The-Glae-Encyclopedia-of-Medicine.pdf', 'total_pages': 759, 'page': 2, 'page_label': '3'}, page_content='STAFF\\nJacqueline L. Longe, Project Editor\\nDeirdre S. Blanchfield, Associate Editor\\nChristine B. Jeryan, Managing Editor\\nDonna Olendorf, Senior Editor\\nStacey Blachford, Associate Editor\\nKate Kretschmann, Melissa C. McDade, Ryan\\nThomason, Assistant Editors\\nMark Springer, Technical Specialist\\nAndrea Lopeman, Programmer/Analyst\\nBarbara J. Yarrow,Manager, Imaging and Multimedia\\nContent\\nRobyn V . Young,Project Manager, Imaging and\\nMultimedia Content\\nDean Dauphinais, Senior Editor, Imaging and'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.10', 'creator': '', 'creationdate': '2017-05-01T10:37:35-07:00', 'moddate': '2017-05-01T10:37:35-07:00', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'source': './data/The-Glae-Encyclopedia-of-Medicine.pdf', 'total_pages': 759, 'page': 2, 'page_label': '3'}, page_content='Dean Dauphinais, Senior Editor, Imaging and\\nMultimedia Content\\nKelly A. Quin, Editor, Imaging and Multimedia Content\\nLeitha Etheridge-Sims, Mary K. Grimes, Dave Oblender,\\nImage Catalogers\\nPamela A. Reed, Imaging Coordinator\\nRandy Bassett, Imaging Supervisor\\nRobert Duncan, Senior Imaging Specialist\\nDan Newell, Imaging Specialist\\nChristine O’Bryan, Graphic Specialist\\nMaria Franklin, Permissions Manager\\nMargaret A. Chamberlain, Permissions Specialist\\nMichelle DiMercurio, Senior Art Director'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.10', 'creator': '', 'creationdate': '2017-05-01T10:37:35-07:00', 'moddate': '2017-05-01T10:37:35-07:00', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'source': './data/The-Glae-Encyclopedia-of-Medicine.pdf', 'total_pages': 759, 'page': 2, 'page_label': '3'}, page_content='Michelle DiMercurio, Senior Art Director\\nMike Logusz, Graphic Artist\\nMary Beth Trimper,Manager, Composition and\\nElectronic Prepress\\nEvi Seoud, Assistant Manager, Composition Purchasing\\nand Electronic Prepress\\nDorothy Maki, Manufacturing Manager\\nWendy Blurton, Senior Manufacturing Specialist\\nThe GALE\\nENCYCLOPEDIA\\nof MEDICINE\\nSECOND EDITION\\nSince this page cannot legibly accommodate all copyright notices, the\\nacknowledgments constitute an extension of the copyright notice.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.10', 'creator': '', 'creationdate': '2017-05-01T10:37:35-07:00', 'moddate': '2017-05-01T10:37:35-07:00', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'source': './data/The-Glae-Encyclopedia-of-Medicine.pdf', 'total_pages': 759, 'page': 2, 'page_label': '3'}, page_content='While every effort has been made to ensure the reliability of the infor-\\nmation presented in this publication, the Gale Group neither guarantees\\nthe accuracy of the data contained herein nor assumes any responsibili-\\nty for errors, omissions or discrepancies. The Gale Group accepts no\\npayment for listing, and inclusion in the publication of any organiza-\\ntion, agency, institution, publication, service, or individual does not\\nimply endorsement of the editor or publisher. Errors brought to the'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.10', 'creator': '', 'creationdate': '2017-05-01T10:37:35-07:00', 'moddate': '2017-05-01T10:37:35-07:00', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'source': './data/The-Glae-Encyclopedia-of-Medicine.pdf', 'total_pages': 759, 'page': 2, 'page_label': '3'}, page_content='attention of the publisher and verified to the satisfaction of the publish-\\ner will be corrected in future editions.\\nThis book is printed on recycled paper that meets Environmental Pro-\\ntection Agency standards.\\nThe paper used in this publication meets the minimum requirements of\\nAmerican National Standard for Information Sciences-Permanence\\nPaper for Printed Library Materials, ANSI Z39.48-1984.\\nThis publication is a creative work fully protected by all applicable'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.10', 'creator': '', 'creationdate': '2017-05-01T10:37:35-07:00', 'moddate': '2017-05-01T10:37:35-07:00', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'source': './data/The-Glae-Encyclopedia-of-Medicine.pdf', 'total_pages': 759, 'page': 2, 'page_label': '3'}, page_content='copyright laws, as well as by misappropriation, trade secret, unfair com-\\npetition, and other applicable laws. The authors and editor of this work\\nhave added value to the underlying factual material herein through one\\nor more of the following: unique and original selection, coordination,\\nexpression, arrangement, and classification of the information.\\nGale Group and design is a trademark used herein under license.\\nAll rights to this publication will be vigorously defended.\\nCopyright © 2002'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.10', 'creator': '', 'creationdate': '2017-05-01T10:37:35-07:00', 'moddate': '2017-05-01T10:37:35-07:00', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'source': './data/The-Glae-Encyclopedia-of-Medicine.pdf', 'total_pages': 759, 'page': 2, 'page_label': '3'}, page_content='Copyright © 2002\\nGale Group\\n27500 Drake Road\\nFarmington Hills, MI 48331-3535\\nAll rights reserved including the right of reproduction in whole or in\\npart in any form.\\nISBN 0-7876-5489-2 (set)\\n0-7876-5490-6 (V ol. 1)\\n0-7876-5491-4 (V ol. 2)\\n0-7876-5492-2 (V ol. 3)\\n0-7876-5493-0 (V ol. 4)\\n0-7876-5494-9 (V ol. 5)\\nPrinted in the United States of America\\n10 9 8 7 6 5 4 3 2 1\\nLibrary of Congress Cataloging-in-Publication Data\\nGale encyclopedia of medicine / Jacqueline L. Longe, editor;'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.10', 'creator': '', 'creationdate': '2017-05-01T10:37:35-07:00', 'moddate': '2017-05-01T10:37:35-07:00', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'source': './data/The-Glae-Encyclopedia-of-Medicine.pdf', 'total_pages': 759, 'page': 2, 'page_label': '3'}, page_content='Deirdre S. Blanchfield, associate editor — 2nd ed.\\np. cm.\\nIncludes bibliographical references and index.\\nContents: V ol. 1. A-B — v. 2. C-F — v. 3.\\nG-M — v. 4. N-S — v. 5. T-Z.\\nISBN 0-7876-5489-2 (set: hardcover) — ISBN 0-7876-5490-6\\n(vol. 1) — ISBN 0-7876-5491-4 (vol. 2) — ISBN 0-7876-5492-2\\n(vol. 3) — ISBN 0-7876-5493-0 (vol. 4) — ISBN 0-7876-5494-9\\n(vol. 5)\\n1. Internal medicine—Encyclopedias. I. Longe, Jacqueline L. \\nII. Blanchfield, Deirdre S. III. Gale Research Company.\\nRC41.G35 2001')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Text Chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 50)\n",
    "\n",
    "dataset = text_splitter.split_documents(medical_docs)\n",
    "dataset[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ddadec",
   "metadata": {},
   "source": [
    "**<span style=\"color: #A1AEB1;\">Embedding & Vectorization</span>**\n",
    "\n",
    "<div style=\"text-align: justify;\">\n",
    "\n",
    "As aforementioned, OpenAI’s services were selected for both embedding generation and Large Language Model (LLM) due to their demonstrated performance, operational stability, and seamless compatibility with the LangChain framework. Thus, the comparative analysis concentrates on three embedding models within the OpenAI ecosystem including text-embedding-3-small, text-embedding-3-large, and text-embedding-ada-002, and I will evaluate their relative effectiveness and suitability for downstream retrieval-augmented generation tasks.\n",
    "\n",
    "The newest ligtweight embedding model from OpenAI, **text-embedding-3-small** model is intended to provide cutting-edge semantic performance with a smaller computing footprint. It provides significantly higher accuracy than older models despite operating at a lower dimensionality, enabling more efficient FAISS indexing and faster similarity search. Empirical testing indicates that this model achieves strong semantic alignment for medical terminology, clinical statements, and multi-sentence medical narratives, making it suitable for large-scale ingestion pipelines.\n",
    "\n",
    "Additionally, **text-embedding-3-large** model is engineered for high-precision retrieval in scenarios where semantic fidelity and recall are prioritized over computational cost. It consistently yields stronger performance in tasks involving long-form medical documents, subtle clinical distinctions, and multi-hop medical reasoning. This makes it particularly valuable for safety-sensitive or high-recall applications such as diagnostic guidance, literature retrieval, and guideline summarization. However, the larger vector dimensionality increases memory usage and index size.\n",
    "\n",
    "Compared with both text-embedding-3 series, the predecessor **text-embedding-ada-002** model previously served as OpenAI’s standard embedding solution. While ada-002 remains functional, it generally exhibits lower semantic resolution, weaker contextual understanding, and higher variance in retrieval quality, especially for domain-specific text such as medical literature and clinical guidelines. Its performance gap is evident across semantic similarity benchmarks and multi-paragraph retrieval tasks. Consequently, ada-002 is no longer preferred for systems requiring high accuracy or domain sensitivity.\n",
    "\n",
    "<span style=\"color: #74B72E;\">The empirical evaluation revealed that **text-embedding-3-small** was chosen for this study because it strikes the best possible balance between cost-effectiveness, computational efficiency, and semantic quality. It minimises index size and offers a high enough retrieval accuracy for medical-domain RAG tasks, making it appropriate for scale deployment and iterative prototyping. Text-embedding-3-large is still a feasible upgrade path for situations needing maximum recall or managing intricate clinical narratives. Despite being compatible with any LLM, OpenAI embeddings perform best when combined with OpenAI LLMs because of cross-component optimisation, shared semantic assumptions, and architectural alignment.</span>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "976e6415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Framework Libraries\n",
    "from langchain_openai import OpenAIEmbeddings                                            \n",
    "from langchain_community.vectorstores import FAISS                                                  # Use to store, index, and search through large collection of vector embeddings efficiently\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d1e60cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API Key from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Create Vector Embeddings\n",
    "embedding_model = OpenAIEmbeddings(model = \"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d88c2fc",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "\n",
    "Throughout the development of the RAG pipeline, several vector databases and ANN (Approximate Nearest Neighbor) indexing frameworks were evaluated, including Pinecone, Weaviate, Milvus, ChromaDB, and HNSWlib, in addition to FAISS. Each solution offers a different balance of performance, scalability, operational complexity, and integration support.\n",
    "\n",
    "**Pinecone** provides a highly scalable, fully managed vector database that delivers strong performance, hybrid search capabilities, and robust metadata filtering. Its primary limitations lie in its proprietary ecosystem and relatively high operational cost, making it less suitable for early-stage experimentation or local development environments.\n",
    "\n",
    "**Weaviate** offers an open-source, schema-driven vector store with built-in hybrid search, modular storage backends, and cloud deployment options. While flexible, it introduces additional configuration overhead and resource consumption, which can be excessive for smaller projects or prototype settings.\n",
    "\n",
    "**Milvus**, designed for distributed vector storage at scale, demonstrates excellent performance for large datasets and production-grade workloads. However, its operational complexity which often requiring container orchestration, specialized storage, and dedicated cluster management makes it heavyweight for lightweight RAG prototypes or academic experimentation.\n",
    "\n",
    "**ChromaDB** serves as a lightweight, developer-friendly local vector store with a simple API and effective metadata filtering. Although suitable for small to medium applications, it lacks the raw search performance and index configurability required for more advanced or computationally demanding retrieval tasks.\n",
    "\n",
    "**HNSWlib**, though extremely fast and easy to use, is limited by its in-memory design and lack of broader database features such as persistence, filtering, or integrated metadata search, making it less suitable as a standalone vector database in structured RAG pipelines.\n",
    "\n",
    "<span style=\"color: #74B72E;\">Compared with alternatives, **FAISS** emerged as the most appropriate choice for the current stage of the project. FAISS offers high-performance vector indexing, GPU acceleration, and fine-grained control over ANN algorithms, while remaining simple to deploy in a local development environment. It is widely used in research and prototyping due to its reliability, mature codebase, and strong ecosystem support. Notably, FAISS integrates seamlessly with the LangChain framework, which allows embeddings, chunk metadata, and retrieval logic to be managed efficiently using LangChain’s vector store abstractions. This compatibility reduces implementation effort and ensures that the RAG pipeline remains modular, extensible, and aligned with industry best practices.</span>\n",
    "\n",
    "In summary, FAISS provides the best balance between performance, simplicity, configurability, and ecosystem support for the current development goals. While future iterations of the medical RAG chatbot may require migration to a distributed or managed vector database, FAISS is well-suited for prototyping, testing retrieval strategies, and validating the overall architecture.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7835171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Embeddings in FAISS\n",
    "vector_database = FAISS.from_documents(dataset, embedding_model)\n",
    "vector_database.save_local(VECTOR_DATABASE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3e80ad",
   "metadata": {},
   "source": [
    "**<span style=\"color: #A1AEB1;\">Retrieval-Augmented Generation (RAG) Prompting</span>**\n",
    "\n",
    "<div style=\"text-align: justify;\">\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
