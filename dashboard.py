# Necessary Libraries for RAG Chain
from langchain.chains.combine_documents import create_stuff_documents_chain     # Combines text chunks into a query, stuffs them together into a single prompt template
from langchain.chains import create_retrieval_chain                             # Creates a Retrieval-Augmented Generation (RAG) chain with a retriever and document chain
from langchain_groq import ChatGroq                                             # Use to run open-source models at very high speed
from langchain import hub                                                       # Hub to pull prompt templates

# Necessary Libraries for Vector Database
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

from dotenv import load_dotenv
import os

# Necessary Libraries for Dashboard User Interface
import streamlit as st

# Load API Key from .env file
load_dotenv()

# Constant Variables
GROQ_MODEL_NAME = "llama-3.1-8b-instant"
GROQ_API_KEY = os.getenv("GROQ_API_KEY")

VECTOR_DATABASE_PATH = "./vector_database/db_faiss"

# === Vector Database Setup ===
@st.cache_resource
def vector_database():
    # Create Vector Embeddings
    embedding_model = HuggingFaceEmbeddings(model_name = "sentence-transformers/all-MiniLM-L6-v2")

    # Load Dataset from FAISS Vector Database
    database = FAISS.load_local(VECTOR_DATABASE_PATH, embedding_model, allow_dangerous_deserialization = True)

    return database

# === Retrieval-Augmented Generation (RAG) Chain Setup ===
@st.cache_resource
def retrieval_augmented_generation_chain():
    # Setup Large Language Model
    model = ChatGroq(model = GROQ_MODEL_NAME, temperature = 0.5, max_tokens = 512, api_key = GROQ_API_KEY)

    # Pull Pre-Build Prompt Template from LangChain Hub
    prompt = hub.pull("langchain-ai/retrieval-qa-chat")

    # Combine Text Chunks with Prompt Template
    docs_chain_combine = create_stuff_documents_chain(llm = model, prompt = prompt)

    # Build a Retrieval-Augmented Generation (RAG) Chain
    database = vector_database()
    rag_chain = create_retrieval_chain(database.as_retriever(search_kwargs = {"k": 3}), docs_chain_combine)

    return rag_chain

# === User Interface Setup ===
# Initialize RAG Chain Model
model = retrieval_augmented_generation_chain()

# Page Configuration
st.set_page_config(page_title = "Artificial Intelligence Medical Chatbot", page_icon = "ðŸ©º", layout = "wide")

# Title of the Dashboard
st.title(body = " Medical Chatbot", anchor = False)

st.markdown(
    """
    <style>
        .assistant-card {
            background-color: #f9fbfd;
            border-left: 5px solid #0a84ff;
            padding: 20px;
            border-radius: 12px;
            margin: 15px 0;
            box-shadow: 0px 2px 6px rgba(0,0,0,0.05);
            font-family: 'Helvetica Neue', sans-serif;
        }
        .assistant-header {
            font-size: 20px;
            font-weight: 600;
            color: #0a84ff;
            margin-bottom: 10px;
        }
        .assistant-section-title {
            font-weight: 600;
            color: #333333;
            margin-top: 10px;
            margin-bottom: 5px;
        }
        .assistant-list {
            margin-left: 20px;
            color: #444444;
            line-height: 1.6;
        }
        .assistant-note {
            color: #6b7280;
            font-style: italic;
            margin-top: 10px;
            font-size: 14px;
        }
    </style>
    """, 
    unsafe_allow_html = True
)

# Chat History Session State
if "messages" not in st.session_state:
    st.session_state.messages = []
    
# Display the History of Chat
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"], unsafe_allow_html = True)
        st.divider()
        st.write("Note: The response is intended for informational purpose and does not replace professional medical consultation. Please consult a qualified healthcare professional for personalised diagnosis or treatment.")
        st.caption("Generated by AI Medical Chatbot")

# Chat Input
if prompt := st.chat_input("Describe the symptoms you are experiencing"):
    # Display User Message
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    with st.chat_message("user"):
        st.markdown(prompt)
        
    if prompt.strip():
        res = model.invoke({"input": prompt})
        ans = res["answer"]

        # Save Assistant Message
        st.session_state.messages.append({"role": "assistant", "content": ans})

        with st.chat_message("assistant"):
            st.markdown(ans, unsafe_allow_html = True)
            st.divider()
            st.write("Note: The response is intended for informational purpose and does not replace professional medical consultation. Please consult a qualified healthcare professional for personalised diagnosis or treatment.")
            st.caption("Generated by AI Medical Chatbot")